# Validaci√≥n del Agente RL en Producci√≥n

Este directorio contiene scripts para validar el comportamiento del agente de Reinforcement Learning en un entorno de producci√≥n con datos reales.

## üìÅ Archivos Disponibles

### 1. `Ejecucion_Agente_production.py` (Script Principal)
**Prop√≥sito**: Validaci√≥n completa del agente RL en producci√≥n con datos reales.

**Caracter√≠sticas**:
- ‚úÖ Usa datos reales de la mina (`MINE-hudbay-TicksTrainRL-TIME-2025-08-01.json`)
- ‚úÖ Carga la Q-table entrenada (`q_table_real.pkl`)
- ‚úÖ Detecta y registra estados nuevos vs conocidos
- ‚úÖ Valida la actualizaci√≥n de la matriz Q
- ‚úÖ Proporciona m√©tricas detalladas de rendimiento
- ‚úÖ Logging completo de todas las decisiones
- ‚úÖ **NUEVO**: An√°lisis detallado de ETAs y rewards como justificaci√≥n
- ‚úÖ **NUEVO**: Comparaci√≥n de todas las opciones disponibles
- ‚úÖ **NUEVO**: Ranking de decisiones basado en rewards esperados

**Uso**:
```bash
python test/Ejecucion_Agente_production.py
```

**Salida**:
- M√©tricas detalladas de rendimiento
- An√°lisis de estados nuevos vs conocidos
- Verificaci√≥n de actualizaci√≥n de Q-table
- Logs completos en `logs/agente_rl_produccion_YYYYMMDD_HHMMSS.log`

### 2. `validacion_estados_nuevos.py` (An√°lisis Espec√≠fico)
**Prop√≥sito**: An√°lisis detallado del comportamiento con estados nuevos vs conocidos.

**Caracter√≠sticas**:
- üîç Compara rendimiento entre estados nuevos y conocidos
- üìä Estad√≠sticas de exploraci√≥n vs explotaci√≥n
- üí∞ An√°lisis de recompensas por tipo de estado
- üìù Ejemplos de estados nuevos encontrados

**Uso**:
```bash
python test/validacion_estados_nuevos.py
```

### 3. `test_agente_produccion.py` (Prueba R√°pida)
**Prop√≥sito**: Prueba r√°pida y simple del agente en producci√≥n.

**Caracter√≠sticas**:
- ‚ö° Procesa solo 10 ticks para prueba r√°pida
- üß™ Salida detallada paso a paso
- üìà M√©tricas b√°sicas de rendimiento
- üîç Verificaci√≥n de actualizaci√≥n de Q-table

**Uso**:
```bash
python test/test_agente_produccion.py
```

### 4. `prueba_agente_con_etas.py` (An√°lisis Detallado de ETAs)
**Prop√≥sito**: An√°lisis detallado de decisiones con informaci√≥n de ETAs y rewards.

**Caracter√≠sticas**:
- üöó **NUEVO**: Muestra ETAs del cami√≥n hacia cada pala
- üí∞ **NUEVO**: Calcula rewards esperados para todas las opciones
- üìä **NUEVO**: Ranking de decisiones basado en rewards
- ‚öñÔ∏è **NUEVO**: An√°lisis ETA vs Reward (si seleccion√≥ la mejor opci√≥n)
- üîç **NUEVO**: Comparaci√≥n detallada de todas las palas disponibles
- üìã Tabla comparativa con Q-values, ETAs, estado de palas y rewards

**Uso**:
```bash
python test/prueba_agente_con_etas.py
```

## üéØ Objetivos de Validaci√≥n

### 1. Validar Estados Nuevos
- **Objetivo**: Verificar que el agente puede manejar estados que no vio durante el entrenamiento
- **M√©trica**: N√∫mero de estados nuevos encontrados y agregados a la Q-table
- **Resultado esperado**: El agente debe poder procesar estados nuevos y agregarlos a su conocimiento

### 2. Validar An√°lisis de ETAs y Rewards
- **Objetivo**: Verificar que el agente toma decisiones considerando ETAs y rewards esperados
- **M√©trica**: Comparaci√≥n entre mejor opci√≥n por reward vs opci√≥n seleccionada
- **Resultado esperado**: El agente debe seleccionar opciones con buenos ETAs y rewards

### 3. Validar Actualizaci√≥n de Q-Table
- **Objetivo**: Confirmar que la matriz Q se actualiza correctamente con nueva informaci√≥n
- **M√©trica**: N√∫mero de Q-values actualizados durante la ejecuci√≥n
- **Resultado esperado**: Los Q-values deben cambiar bas√°ndose en las recompensas obtenidas

### 4. Validar Comportamiento Exploraci√≥n/Explotaci√≥n
- **Objetivo**: Verificar el balance entre exploraci√≥n y explotaci√≥n
- **M√©trica**: Tasa de exploraci√≥n vs explotaci√≥n
- **Resultado esperado**: El agente debe explorar estados nuevos y explotar estados conocidos

### 5. Validar Rendimiento en Producci√≥n
- **Objetivo**: Medir el rendimiento del agente con datos reales
- **M√©trica**: Recompensa total y promedio por decisi√≥n
- **Resultado esperado**: El agente debe mantener o mejorar su rendimiento

## üìä M√©tricas Clave

### M√©tricas Generales
- **Total de ticks procesados**: N√∫mero de ticks analizados
- **Total de camiones optimizados**: Camiones que requirieron decisi√≥n del agente
- **Total de camiones fijos**: Camiones que no necesitaron optimizaci√≥n
- **Estados √∫nicos vistos**: Diferentes estados encontrados

### M√©tricas de Aprendizaje
- **Estados nuevos encontrados**: Estados no vistos durante el entrenamiento
- **Q-values actualizados**: N√∫mero de actualizaciones en la matriz Q
- **Decisiones por exploraci√≥n**: Acciones tomadas aleatoriamente
- **Decisiones por explotaci√≥n**: Acciones basadas en Q-values conocidos

### M√©tricas de Rendimiento
- **Recompensa total acumulada**: Suma de todas las recompensas obtenidas
- **Recompensa promedio por cami√≥n**: Rendimiento promedio por decisi√≥n
- **Distribuci√≥n de acciones por pala**: C√≥mo se distribuyen las asignaciones

## üîß Configuraci√≥n

### Rutas de Archivos
```python
# Q-table entrenada
QTABLE_PATH = "agent/q_table_real.pkl"

# Datos reales de producci√≥n
TICK_FILE = "agent/MINE-hudbay-TicksTrainRL-TIME-2025-08-01.json"

# Palas disponibles
SHOVEL_NAMES = ['PH002', 'EX004', 'PH003', 'PH001', 'CF001', 'CF002']
```

### Par√°metros del Agente
- **Alpha (tasa de aprendizaje)**: 0.1
- **Gamma (factor de descuento)**: 0.9
- **Epsilon (tasa de exploraci√≥n)**: 1.0
- **Epsilon decay**: 0.1
- **Epsilon min**: 0.01

## üìù Interpretaci√≥n de Resultados

### ‚úÖ Comportamiento Esperado
1. **Estados nuevos**: El agente debe encontrar y procesar estados nuevos
2. **Actualizaci√≥n Q-table**: Los Q-values deben actualizarse con nueva informaci√≥n
3. **Exploraci√≥n balanceada**: Mezcla de exploraci√≥n y explotaci√≥n
4. **Rendimiento estable**: Recompensas consistentes o mejorando

### ‚ö†Ô∏è Se√±ales de Problemas
1. **No se encuentran estados nuevos**: Posible sobreajuste o datos muy similares
2. **Q-values no se actualizan**: Problema en el algoritmo de actualizaci√≥n
3. **Solo exploraci√≥n**: Epsilon demasiado alto
4. **Solo explotaci√≥n**: Epsilon demasiado bajo
5. **Recompensas muy bajas**: Problema en la funci√≥n de recompensa

## üöÄ Pr√≥ximos Pasos

1. **Ejecutar validaci√≥n completa**: Usar `Ejecucion_Agente_production.py`
2. **Analizar resultados**: Revisar m√©tricas y logs
3. **Ajustar par√°metros**: Si es necesario, modificar epsilon, alpha, etc.
4. **Validar con m√°s datos**: Probar con diferentes conjuntos de datos
5. **Monitoreo continuo**: Implementar validaci√≥n autom√°tica en producci√≥n

## üìû Soporte

Para problemas o preguntas sobre la validaci√≥n:
1. Revisar los logs generados
2. Verificar que los archivos de datos existen
3. Confirmar que la Q-table est√° correctamente entrenada
4. Revisar las m√©tricas de rendimiento 